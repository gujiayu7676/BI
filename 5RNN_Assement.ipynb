{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "目前我们可以有\n",
    "## RNN Class\n",
    "RNNs 很容易实现，接受一个$x$ vector作为输入并返回一个$y$ vector。 只不过输出的内容不仅仅与当前的输入有关，还与过去的输入是相关的。那么我们可以定义一个RNN的class，通过以下调用方式来实现一次迭代："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每调用一次`step`， state向量 $h$ 就会被更新一次， 请同学们根据课上所讲内容，完成RNN的定义，并构建一个多层RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self,input_dim,hidden_dim,output_dim):\n",
    "        self.hs = {}\n",
    "        self.input_dim = input_dim  #输入层维度\n",
    "        self.hidden_dim = hidden_dim  #隐层维度\n",
    "        self.output_dim = output_dim  #输出层维度\n",
    "        self.hprev = np.zeros((hidden_dim,1))\n",
    "        self.hs[-1] = self.hprev  #h初始值\n",
    "        \n",
    "    def print_RNN(self):\n",
    "        print(\"输入层维度：{0}，隐层维度：{1}，输出层维度：{2}\\n\".format(self.input_dim,self.hidden_dim,self.output_dim))\n",
    "        \n",
    "    def step(self,t,x):\n",
    "        self.Whx = np.random.randn(self.hidden_dim, self.input_dim)# input to hidden\n",
    "        self.Whh = np.random.randn(self.hidden_dim, self.hidden_dim) # hidden to hidden\n",
    "        self.Why = np.random.randn(self.output_dim, self.hidden_dim) # hidden to output\n",
    "        self.bh = np.zeros((self.hidden_dim, 1)) # hidden bias\n",
    "        self.by = np.zeros((self.output_dim, 1)) # output bias\n",
    "        # update the hidden state\n",
    "        self.hs[t] =  np.tanh(np.dot(self.Whh,self.hs[t-1]) + self.bh + np.dot(self.Whx,x))\n",
    "        # compute the output vectors\n",
    "        y = np.dot(self.Why,self.hs[t]) + self.by\n",
    "        return y,self.hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入层one_hot编码：[[0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "输入层维度：4，隐层维度：3，输出层维度：4\n",
      "\n",
      "第0层输出拉平后:[ 1.13392716  0.3900951  -0.03377654 -0.56116103]\n",
      ",hs:{-1: array([[0.],\n",
      "       [0.],\n",
      "       [0.]]), 0: array([[ 0.97468086],\n",
      "       [-0.53117275],\n",
      "       [-0.80720381]])}\n",
      "\n",
      "****************************************************\n",
      "第1层输出拉平后:[ 1.36769746  1.47565185  1.25032552 -0.76471354]\n",
      ",hs:{-1: array([[0.],\n",
      "       [0.],\n",
      "       [0.]]), 0: array([[ 0.97468086],\n",
      "       [-0.53117275],\n",
      "       [-0.80720381]]), 1: array([[-0.87020032],\n",
      "       [-0.28302146],\n",
      "       [-0.92763278]])}\n",
      "\n",
      "****************************************************\n",
      "第2层输出拉平后:[ 0.29863054  3.01268666 -0.88876414 -3.99118555]\n",
      ",hs:{-1: array([[0.],\n",
      "       [0.],\n",
      "       [0.]]), 0: array([[ 0.97468086],\n",
      "       [-0.53117275],\n",
      "       [-0.80720381]]), 1: array([[-0.87020032],\n",
      "       [-0.28302146],\n",
      "       [-0.92763278]]), 2: array([[ 0.90682082],\n",
      "       [ 0.77305373],\n",
      "       [-0.97965883]])}\n",
      "\n",
      "****************************************************\n",
      "第3层输出拉平后:[-1.68833162 -1.95337979 -1.76070161 -1.52418642]\n",
      ",hs:{-1: array([[0.],\n",
      "       [0.],\n",
      "       [0.]]), 0: array([[ 0.97468086],\n",
      "       [-0.53117275],\n",
      "       [-0.80720381]]), 1: array([[-0.87020032],\n",
      "       [-0.28302146],\n",
      "       [-0.92763278]]), 2: array([[ 0.90682082],\n",
      "       [ 0.77305373],\n",
      "       [-0.97965883]]), 3: array([[-0.83748213],\n",
      "       [-0.90631391],\n",
      "       [-0.61086145]])}\n",
      "\n",
      "****************************************************\n",
      "第4层输出拉平后:[-0.25072783 -0.83357694  0.74789431 -3.30096294]\n",
      ",hs:{-1: array([[0.],\n",
      "       [0.],\n",
      "       [0.]]), 0: array([[ 0.97468086],\n",
      "       [-0.53117275],\n",
      "       [-0.80720381]]), 1: array([[-0.87020032],\n",
      "       [-0.28302146],\n",
      "       [-0.92763278]]), 2: array([[ 0.90682082],\n",
      "       [ 0.77305373],\n",
      "       [-0.97965883]]), 3: array([[-0.83748213],\n",
      "       [-0.90631391],\n",
      "       [-0.61086145]]), 4: array([[-0.98625223],\n",
      "       [-0.9636144 ],\n",
      "       [-0.98033851]])}\n",
      "\n",
      "****************************************************\n"
     ]
    }
   ],
   "source": [
    "# Going deep\n",
    "#以课件Character-Level Language Models，输入的是[\"h\",\"e\",\"l\",\"l\",\"o\"]的one_hot编码，timestep为5，查看每一层的输出\n",
    "\n",
    "inputs = np.array([['h'],['e'],['l'],['l'],['o']])\n",
    "cat_encoder = OneHotEncoder(sparse=False)#稀疏\n",
    "inputs_1hot= cat_encoder.fit_transform(inputs)\n",
    "print(\"输入层one_hot编码：{0}\".format(inputs_1hot))\n",
    "inputs_1hot = list(map(lambda x:x.reshape(4,1),inputs_1hot)) #每个one_hot编码转置操作\n",
    "input_dim = inputs_1hot[0].shape[0]  #输入层维度\n",
    "out_dim = input_dim  #输出层维度\n",
    "hidden_dim = 3  #隐层维度\n",
    "rnn = RNN(input_dim,hidden_dim,out_dim)\n",
    "rnn.print_RNN()\n",
    "\n",
    "#xt is an input vector, y is the RNN's output vector\n",
    "for t,xt in enumerate(inputs_1hot):\n",
    "    y,hs = rnn.step(t,xt)\n",
    "    print(\"第{0}层输出拉平后:{1}\\n,hs:{2}\\n\".format(t,y.ravel(),hs))\n",
    "    print('****************************************************')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结合课堂代码，自己实现一个character-level 的RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnModel(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size,layer_num):\n",
    "        super().__init__()\n",
    "        self.rnnLayer = nn.RNN(input_size,hidden_size,layer_num,batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size,output_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out,_ = self.rnnLayer(x)\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入层one_hot编码：[array([[0., 1., 0., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 0., 1.]]), array([[1., 0., 0., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 0., 1.],\n",
      "       [0., 1., 0., 0.]])]\n",
      "标签：[tensor([0, 2, 2, 3, 1]), tensor([2, 2, 3, 1, 0])]\n"
     ]
    }
   ],
   "source": [
    "#自定义数据集：输入数据集包含2个样本，一个是hello，另外一个是elloh\n",
    "inputs_array = np.array([[['h'],['e'],['l'],['l'],['o']],\n",
    "                        [['e'],['l'],['l'],['o'],['h']]]) #target 就是 elloh和llohe\n",
    "inputs_list = []#转换成one_hot编码形式\n",
    "target_list = []\n",
    "for x in inputs_array:\n",
    "    cat_encoder = OneHotEncoder(sparse=False)#稀疏\n",
    "    inputs_1hot= cat_encoder.fit_transform(x)\n",
    "    inputs_list.append(inputs_1hot)\n",
    "    target_order = [1,2,3,4,0]\n",
    "    target_1hot = torch.FloatTensor(inputs_1hot[target_order])\n",
    "    _,target = torch.max(target_1hot, 1) \n",
    "    target_list.append(target)\n",
    "print(\"输入层one_hot编码：{0}\".format(inputs_list))\n",
    "print(\"标签：{0}\".format(target_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RnnModel(\n",
       "  (rnnLayer): RNN(4, 3, batch_first=True)\n",
       "  (fc): Linear(in_features=3, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 1 \n",
    "input_size= 4 #输入数据维度\n",
    "hidden_size = 3 #h向量维度\n",
    "output_size = 4 #输出数据维度\n",
    "layer_num = 1 #RNN层数\n",
    "seq_len = 5 #序列数\n",
    "rnn = RnnModel(4,3,4,1)  #input_size= 4,hidden_size = 3,output_size = 4,layer_num = 1\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn.parameters(),lr = 1e-1)\n",
    "rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "第1个样本预测值:tensor([2, 2, 1, 1, 2])\n",
      "第1个样本标签值:tensor([0, 2, 2, 3, 1])\n",
      "第2个样本预测值:tensor([2, 2, 2, 2, 2])\n",
      "第2个样本标签值:tensor([2, 2, 3, 1, 0])\n",
      "Loss:1.3762,Acc:0.3000\n",
      "***************************************\n",
      "Epoch 2/20\n",
      "第1个样本预测值:tensor([2, 2, 2, 2, 2])\n",
      "第1个样本标签值:tensor([0, 2, 2, 3, 1])\n",
      "第2个样本预测值:tensor([2, 2, 2, 2, 2])\n",
      "第2个样本标签值:tensor([2, 2, 3, 1, 0])\n",
      "Loss:1.2341,Acc:0.4000\n",
      "***************************************\n",
      "Epoch 3/20\n",
      "第1个样本预测值:tensor([0, 2, 2, 2, 2])\n",
      "第1个样本标签值:tensor([0, 2, 2, 3, 1])\n",
      "第2个样本预测值:tensor([2, 2, 2, 2, 0])\n",
      "第2个样本标签值:tensor([2, 2, 3, 1, 0])\n",
      "Loss:1.0592,Acc:0.6000\n",
      "***************************************\n",
      "Epoch 4/20\n",
      "第1个样本预测值:tensor([0, 2, 2, 2, 1])\n",
      "第1个样本标签值:tensor([0, 2, 2, 3, 1])\n",
      "第2个样本预测值:tensor([2, 2, 2, 1, 0])\n",
      "第2个样本标签值:tensor([2, 2, 3, 1, 0])\n",
      "Loss:0.8499,Acc:0.8000\n",
      "***************************************\n",
      "Epoch 5/20\n",
      "第1个样本预测值:tensor([0, 2, 2, 2, 1])\n",
      "第1个样本标签值:tensor([0, 2, 2, 3, 1])\n",
      "第2个样本预测值:tensor([2, 2, 2, 1, 0])\n",
      "第2个样本标签值:tensor([2, 2, 3, 1, 0])\n",
      "Loss:0.6241,Acc:0.8000\n",
      "***************************************\n",
      "Epoch 6/20\n",
      "第1个样本预测值:tensor([0, 2, 2, 2, 1])\n",
      "第1个样本标签值:tensor([0, 2, 2, 3, 1])\n",
      "第2个样本预测值:tensor([2, 2, 3, 1, 0])\n",
      "第2个样本标签值:tensor([2, 2, 3, 1, 0])\n",
      "Loss:0.4538,Acc:0.9000\n",
      "***************************************\n",
      "Epoch 7/20\n",
      "第1个样本预测值:tensor([0, 2, 2, 3, 1])\n",
      "第1个样本标签值:tensor([0, 2, 2, 3, 1])\n",
      "第2个样本预测值:tensor([2, 2, 3, 1, 0])\n",
      "第2个样本标签值:tensor([2, 2, 3, 1, 0])\n",
      "Loss:0.3206,Acc:1.0000\n",
      "***************************************\n",
      "Epoch 8/20\n",
      "第1个样本预测值:tensor([0, 2, 2, 3, 1])\n",
      "第1个样本标签值:tensor([0, 2, 2, 3, 1])\n",
      "第2个样本预测值:tensor([2, 2, 3, 1, 0])\n",
      "第2个样本标签值:tensor([2, 2, 3, 1, 0])\n",
      "Loss:0.2128,Acc:1.0000\n",
      "***************************************\n",
      "Epoch 9/20\n",
      "第1个样本预测值:tensor([0, 2, 2, 3, 1])\n",
      "第1个样本标签值:tensor([0, 2, 2, 3, 1])\n",
      "第2个样本预测值:tensor([2, 2, 3, 1, 0])\n",
      "第2个样本标签值:tensor([2, 2, 3, 1, 0])\n",
      "Loss:0.1448,Acc:1.0000\n",
      "***************************************\n",
      "Epoch 10/20\n",
      "第1个样本预测值:tensor([0, 2, 2, 3, 1])\n",
      "第1个样本标签值:tensor([0, 2, 2, 3, 1])\n",
      "第2个样本预测值:tensor([2, 2, 3, 1, 0])\n",
      "第2个样本标签值:tensor([2, 2, 3, 1, 0])\n",
      "Loss:0.0978,Acc:1.0000\n",
      "***************************************\n",
      "Epoch 11/20\n",
      "第1个样本预测值:tensor([0, 2, 2, 3, 1])\n",
      "第1个样本标签值:tensor([0, 2, 2, 3, 1])\n",
      "第2个样本预测值:tensor([2, 2, 3, 1, 0])\n",
      "第2个样本标签值:tensor([2, 2, 3, 1, 0])\n",
      "Loss:0.0667,Acc:1.0000\n",
      "***************************************\n",
      "Epoch 12/20\n",
      "第1个样本预测值:tensor([0, 2, 2, 3, 1])\n",
      "第1个样本标签值:tensor([0, 2, 2, 3, 1])\n",
      "第2个样本预测值:tensor([2, 2, 3, 1, 0])\n",
      "第2个样本标签值:tensor([2, 2, 3, 1, 0])\n",
      "Loss:0.0476,Acc:1.0000\n",
      "***************************************\n",
      "Epoch 13/20\n",
      "第1个样本预测值:tensor([0, 2, 2, 3, 1])\n",
      "第1个样本标签值:tensor([0, 2, 2, 3, 1])\n",
      "第2个样本预测值:tensor([2, 2, 3, 1, 0])\n",
      "第2个样本标签值:tensor([2, 2, 3, 1, 0])\n",
      "Loss:0.0356,Acc:1.0000\n",
      "***************************************\n",
      "Epoch 14/20\n",
      "第1个样本预测值:tensor([0, 2, 2, 3, 1])\n",
      "第1个样本标签值:tensor([0, 2, 2, 3, 1])\n",
      "第2个样本预测值:tensor([2, 2, 3, 1, 0])\n",
      "第2个样本标签值:tensor([2, 2, 3, 1, 0])\n",
      "Loss:0.0276,Acc:1.0000\n",
      "***************************************\n",
      "Epoch 15/20\n",
      "第1个样本预测值:tensor([0, 2, 2, 3, 1])\n",
      "第1个样本标签值:tensor([0, 2, 2, 3, 1])\n",
      "第2个样本预测值:tensor([2, 2, 3, 1, 0])\n",
      "第2个样本标签值:tensor([2, 2, 3, 1, 0])\n",
      "Loss:0.0222,Acc:1.0000\n",
      "***************************************\n",
      "Epoch 16/20\n",
      "第1个样本预测值:tensor([0, 2, 2, 3, 1])\n",
      "第1个样本标签值:tensor([0, 2, 2, 3, 1])\n",
      "第2个样本预测值:tensor([2, 2, 3, 1, 0])\n",
      "第2个样本标签值:tensor([2, 2, 3, 1, 0])\n",
      "Loss:0.0184,Acc:1.0000\n",
      "***************************************\n",
      "Epoch 17/20\n",
      "第1个样本预测值:tensor([0, 2, 2, 3, 1])\n",
      "第1个样本标签值:tensor([0, 2, 2, 3, 1])\n",
      "第2个样本预测值:tensor([2, 2, 3, 1, 0])\n",
      "第2个样本标签值:tensor([2, 2, 3, 1, 0])\n",
      "Loss:0.0156,Acc:1.0000\n",
      "***************************************\n",
      "Epoch 18/20\n",
      "第1个样本预测值:tensor([0, 2, 2, 3, 1])\n",
      "第1个样本标签值:tensor([0, 2, 2, 3, 1])\n",
      "第2个样本预测值:tensor([2, 2, 3, 1, 0])\n",
      "第2个样本标签值:tensor([2, 2, 3, 1, 0])\n",
      "Loss:0.0135,Acc:1.0000\n",
      "***************************************\n",
      "Epoch 19/20\n",
      "第1个样本预测值:tensor([0, 2, 2, 3, 1])\n",
      "第1个样本标签值:tensor([0, 2, 2, 3, 1])\n",
      "第2个样本预测值:tensor([2, 2, 3, 1, 0])\n",
      "第2个样本标签值:tensor([2, 2, 3, 1, 0])\n",
      "Loss:0.0119,Acc:1.0000\n",
      "***************************************\n",
      "Epoch 20/20\n",
      "第1个样本预测值:tensor([0, 2, 2, 3, 1])\n",
      "第1个样本标签值:tensor([0, 2, 2, 3, 1])\n",
      "第2个样本预测值:tensor([2, 2, 3, 1, 0])\n",
      "第2个样本标签值:tensor([2, 2, 3, 1, 0])\n",
      "Loss:0.0106,Acc:1.0000\n",
      "***************************************\n"
     ]
    }
   ],
   "source": [
    "epoch_num = 20\n",
    "for epoch in range(epoch_num):\n",
    "    print('Epoch {}/{}'.format(epoch+1, epoch_num))\n",
    "    running_loss = 0.0\n",
    "    running_acc =0.0\n",
    "    for i,x_input in enumerate(inputs_list):\n",
    "        optimizer.zero_grad()\n",
    "        inputs = torch.FloatTensor(x_input).reshape(1,5,4) # batch_size = 1 ,seq_len = 5,input_size= 4\n",
    "        target = target_list[i]\n",
    "        output = rnn(inputs)\n",
    "        output = output.reshape(5,4) #去掉batch_size这个维度 这样和target的维度可以匹配上\n",
    "        _,preds = torch.max(output,1)\n",
    "        running_acc += torch.sum(preds == target).double()/target.size()[0]\n",
    "        loss = criterion(output,target)\n",
    "        running_loss += loss.item()\n",
    "        print(\"第{0}个样本预测值:{1}\".format(i+1,preds))\n",
    "        print(\"第{0}个样本标签值:{1}\".format(i+1,target))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    epoch_loss = running_loss/len(inputs_list)\n",
    "    epoch_acc = running_acc/len(inputs_list)\n",
    "    print(\"Loss:{:.4f},Acc:{:.4f}\".format(epoch_loss,epoch_acc))\n",
    "    print(\"***************************************\")\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Class （Optional）\n",
    "自定义一个LSTM网络并进行训练， 对比simple RNN的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lstm(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size,layer_num):\n",
    "        super().__init__()\n",
    "        self.lstmlayer = nn.LSTM(input_size,hidden_size,layer_num,batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size,output_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out,_ = self.lstmlayer(x)\n",
    "        batch,seq,hidden = out.size()\n",
    "        out = out.view(seq*batch,hidden)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入层one_hot编码：[array([[0., 1., 0., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 0., 1.]]), array([[1., 0., 0., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 0., 1.],\n",
      "       [0., 1., 0., 0.]])]\n",
      "标签：[tensor([0, 2, 2, 3, 1]), tensor([2, 2, 3, 1, 0])]\n"
     ]
    }
   ],
   "source": [
    "#自定义数据集：输入数据集包含2个样本，一个是hello，另外一个是elloh\n",
    "inputs_array = np.array([[['h'],['e'],['l'],['l'],['o']],\n",
    "                        [['e'],['l'],['l'],['o'],['h']]]) #target 就是 elloh和llohe\n",
    "inputs_list = []#转换成one_hot编码形式\n",
    "target_list = []\n",
    "for x in inputs_array:\n",
    "    cat_encoder = OneHotEncoder(sparse=False)#稀疏\n",
    "    inputs_1hot= cat_encoder.fit_transform(x)\n",
    "    inputs_list.append(inputs_1hot)\n",
    "    target_order = [1,2,3,4,0]\n",
    "    target_1hot = torch.FloatTensor(inputs_1hot[target_order])\n",
    "    _,target = torch.max(target_1hot, 1) \n",
    "    target_list.append(target)\n",
    "print(\"输入层one_hot编码：{0}\".format(inputs_list))\n",
    "print(\"标签：{0}\".format(target_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lstm(\n",
       "  (lstmlayer): LSTM(4, 3, batch_first=True)\n",
       "  (fc): Linear(in_features=3, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 1 \n",
    "input_size= 4 #输入数据维度\n",
    "hidden_size = 3 #h向量维度\n",
    "output_size = 4 #输出数据维度\n",
    "layer_num = 1 #RNN层数\n",
    "seq_len = 5 #序列数\n",
    "lstm = Lstm(4,3,4,1)  #input_size= 4,hidden_size = 3,output_size = 4,layer_num = 1\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(lstm.parameters(),lr = 1e-1)\n",
    "lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "第1个样本预测值:tensor([0, 0, 0, 0, 0])\n",
      "第1个样本标签值:tensor([0, 2, 2, 3, 1])\n",
      "第2个样本预测值:tensor([2, 2, 2, 2, 0])\n",
      "第2个样本标签值:tensor([2, 2, 3, 1, 0])\n",
      "Loss:1.3726,Acc:0.4000\n",
      "***************************************\n",
      "Epoch 2/20\n",
      "第1个样本预测值:tensor([2, 2, 2, 2, 2])\n",
      "第1个样本标签值:tensor([0, 2, 2, 3, 1])\n",
      "第2个样本预测值:tensor([2, 2, 2, 2, 0])\n",
      "第2个样本标签值:tensor([2, 2, 3, 1, 0])\n",
      "Loss:1.2297,Acc:0.5000\n",
      "***************************************\n",
      "Epoch 3/20\n",
      "第1个样本预测值:tensor([2, 2, 2, 2, 2])\n",
      "第1个样本标签值:tensor([0, 2, 2, 3, 1])\n",
      "第2个样本预测值:tensor([2, 2, 2, 2, 0])\n",
      "第2个样本标签值:tensor([2, 2, 3, 1, 0])\n",
      "Loss:1.0553,Acc:0.5000\n",
      "***************************************\n",
      "Epoch 4/20\n",
      "第1个样本预测值:tensor([1, 2, 2, 2, 1])\n",
      "第1个样本标签值:tensor([0, 2, 2, 3, 1])\n",
      "第2个样本预测值:tensor([2, 2, 2, 1, 1])\n",
      "第2个样本标签值:tensor([2, 2, 3, 1, 0])\n",
      "Loss:0.8720,Acc:0.6000\n",
      "***************************************\n",
      "Epoch 5/20\n",
      "第1个样本预测值:tensor([1, 2, 2, 2, 1])\n",
      "第1个样本标签值:tensor([0, 2, 2, 3, 1])\n",
      "第2个样本预测值:tensor([2, 2, 3, 1, 1])\n",
      "第2个样本标签值:tensor([2, 2, 3, 1, 0])\n",
      "Loss:0.7381,Acc:0.7000\n",
      "***************************************\n",
      "Epoch 6/20\n",
      "第1个样本预测值:tensor([1, 2, 2, 3, 1])\n",
      "第1个样本标签值:tensor([0, 2, 2, 3, 1])\n",
      "第2个样本预测值:tensor([2, 2, 3, 1, 1])\n",
      "第2个样本标签值:tensor([2, 2, 3, 1, 0])\n",
      "Loss:0.6436,Acc:0.8000\n",
      "***************************************\n",
      "Epoch 7/20\n",
      "第1个样本预测值:tensor([1, 2, 2, 3, 1])\n",
      "第1个样本标签值:tensor([0, 2, 2, 3, 1])\n",
      "第2个样本预测值:tensor([2, 2, 3, 1, 1])\n",
      "第2个样本标签值:tensor([2, 2, 3, 1, 0])\n",
      "Loss:0.5541,Acc:0.8000\n",
      "***************************************\n",
      "Epoch 8/20\n",
      "第1个样本预测值:tensor([1, 2, 2, 3, 1])\n",
      "第1个样本标签值:tensor([0, 2, 2, 3, 1])\n",
      "第2个样本预测值:tensor([2, 2, 3, 1, 1])\n",
      "第2个样本标签值:tensor([2, 2, 3, 1, 0])\n",
      "Loss:0.4660,Acc:0.8000\n",
      "***************************************\n",
      "Epoch 9/20\n",
      "第1个样本预测值:tensor([0, 2, 2, 3, 1])\n",
      "第1个样本标签值:tensor([0, 2, 2, 3, 1])\n",
      "第2个样本预测值:tensor([2, 2, 3, 1, 0])\n",
      "第2个样本标签值:tensor([2, 2, 3, 1, 0])\n",
      "Loss:0.3827,Acc:1.0000\n",
      "***************************************\n",
      "Epoch 10/20\n",
      "第1个样本预测值:tensor([0, 2, 2, 3, 1])\n",
      "第1个样本标签值:tensor([0, 2, 2, 3, 1])\n",
      "第2个样本预测值:tensor([2, 2, 3, 1, 0])\n",
      "第2个样本标签值:tensor([2, 2, 3, 1, 0])\n",
      "Loss:0.3092,Acc:1.0000\n",
      "***************************************\n",
      "Epoch 11/20\n",
      "第1个样本预测值:tensor([0, 2, 2, 3, 1])\n",
      "第1个样本标签值:tensor([0, 2, 2, 3, 1])\n",
      "第2个样本预测值:tensor([2, 2, 3, 1, 0])\n",
      "第2个样本标签值:tensor([2, 2, 3, 1, 0])\n",
      "Loss:0.2504,Acc:1.0000\n",
      "***************************************\n",
      "Epoch 12/20\n",
      "第1个样本预测值:tensor([0, 2, 2, 3, 1])\n",
      "第1个样本标签值:tensor([0, 2, 2, 3, 1])\n",
      "第2个样本预测值:tensor([2, 2, 3, 1, 0])\n",
      "第2个样本标签值:tensor([2, 2, 3, 1, 0])\n",
      "Loss:0.1999,Acc:1.0000\n",
      "***************************************\n",
      "Epoch 13/20\n",
      "第1个样本预测值:tensor([0, 2, 2, 3, 1])\n",
      "第1个样本标签值:tensor([0, 2, 2, 3, 1])\n",
      "第2个样本预测值:tensor([2, 2, 3, 1, 0])\n",
      "第2个样本标签值:tensor([2, 2, 3, 1, 0])\n",
      "Loss:0.1605,Acc:1.0000\n",
      "***************************************\n",
      "Epoch 14/20\n",
      "第1个样本预测值:tensor([0, 2, 2, 3, 1])\n",
      "第1个样本标签值:tensor([0, 2, 2, 3, 1])\n",
      "第2个样本预测值:tensor([2, 2, 3, 1, 0])\n",
      "第2个样本标签值:tensor([2, 2, 3, 1, 0])\n",
      "Loss:0.1321,Acc:1.0000\n",
      "***************************************\n",
      "Epoch 15/20\n",
      "第1个样本预测值:tensor([0, 2, 2, 3, 1])\n",
      "第1个样本标签值:tensor([0, 2, 2, 3, 1])\n",
      "第2个样本预测值:tensor([2, 2, 3, 1, 0])\n",
      "第2个样本标签值:tensor([2, 2, 3, 1, 0])\n",
      "Loss:0.1079,Acc:1.0000\n",
      "***************************************\n",
      "Epoch 16/20\n",
      "第1个样本预测值:tensor([0, 2, 2, 3, 1])\n",
      "第1个样本标签值:tensor([0, 2, 2, 3, 1])\n",
      "第2个样本预测值:tensor([2, 2, 3, 1, 0])\n",
      "第2个样本标签值:tensor([2, 2, 3, 1, 0])\n",
      "Loss:0.0924,Acc:1.0000\n",
      "***************************************\n",
      "Epoch 17/20\n",
      "第1个样本预测值:tensor([0, 2, 2, 3, 1])\n",
      "第1个样本标签值:tensor([0, 2, 2, 3, 1])\n",
      "第2个样本预测值:tensor([2, 2, 3, 1, 0])\n",
      "第2个样本标签值:tensor([2, 2, 3, 1, 0])\n",
      "Loss:0.0774,Acc:1.0000\n",
      "***************************************\n",
      "Epoch 18/20\n",
      "第1个样本预测值:tensor([0, 2, 2, 3, 1])\n",
      "第1个样本标签值:tensor([0, 2, 2, 3, 1])\n",
      "第2个样本预测值:tensor([2, 2, 3, 1, 0])\n",
      "第2个样本标签值:tensor([2, 2, 3, 1, 0])\n",
      "Loss:0.0641,Acc:1.0000\n",
      "***************************************\n",
      "Epoch 19/20\n",
      "第1个样本预测值:tensor([0, 2, 2, 3, 1])\n",
      "第1个样本标签值:tensor([0, 2, 2, 3, 1])\n",
      "第2个样本预测值:tensor([2, 2, 3, 1, 0])\n",
      "第2个样本标签值:tensor([2, 2, 3, 1, 0])\n",
      "Loss:0.0557,Acc:1.0000\n",
      "***************************************\n",
      "Epoch 20/20\n",
      "第1个样本预测值:tensor([0, 2, 2, 3, 1])\n",
      "第1个样本标签值:tensor([0, 2, 2, 3, 1])\n",
      "第2个样本预测值:tensor([2, 2, 3, 1, 0])\n",
      "第2个样本标签值:tensor([2, 2, 3, 1, 0])\n",
      "Loss:0.0464,Acc:1.0000\n",
      "***************************************\n"
     ]
    }
   ],
   "source": [
    "epoch_num = 20\n",
    "for epoch in range(epoch_num):\n",
    "    print('Epoch {}/{}'.format(epoch+1, epoch_num))\n",
    "    running_loss = 0.0\n",
    "    running_acc =0.0\n",
    "    for i,x_input in enumerate(inputs_list):\n",
    "        optimizer.zero_grad()\n",
    "        inputs = torch.FloatTensor(x_input).reshape(1,5,4) # batch_size = 1 ,seq_len = 5,input_size= 4\n",
    "        target = target_list[i]\n",
    "        output = lstm(inputs)\n",
    "        output = output.reshape(5,4) #去掉batch_size这个维度 这样和target的维度可以匹配上\n",
    "        _,preds = torch.max(output,1)\n",
    "        running_acc += torch.sum(preds == target).double()/target.size()[0]\n",
    "        loss = criterion(output,target)\n",
    "        running_loss += loss.item()\n",
    "        print(\"第{0}个样本预测值:{1}\".format(i+1,preds))\n",
    "        print(\"第{0}个样本标签值:{1}\".format(i+1,target))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    epoch_loss = running_loss/len(inputs_list)\n",
    "    epoch_acc = running_acc/len(inputs_list)\n",
    "    print(\"Loss:{:.4f},Acc:{:.4f}\".format(epoch_loss,epoch_acc))\n",
    "    print(\"***************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单LSTM和简单RNN对比后，RNN的loss下降更快，最后一个loss最小,LSTM优势并没有体现出来。\n",
    "原因应该是输入的字母序列太短，LSTM擅长长期记忆的优势体现不出来。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "266px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
